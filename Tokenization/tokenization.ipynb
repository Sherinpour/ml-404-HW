{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c89e6a",
   "metadata": {},
   "source": [
    "**WordPiece**\n",
    "\n",
    "The goal of WordPiece is to build a fixed vocabulary \\(V\\) of subwords such that the likelihood of the training corpus under a simple model is maximized.\n",
    "\n",
    "Formally, if a piece of text is segmented into tokens \\(t_1, t_2, \\ldots, t_m\\), the model assumes:\n",
    "\n",
    "$$\n",
    "P(\\text{text}) \\approx \\prod_{i=1}^{m} P(t_i)\n",
    "$$\n",
    "\n",
    "We want to find a vocabulary \\(V\\) and probabilities \\(P(t)\\) that maximize the likelihood of the training data (or equivalently minimize the negative log-likelihood).\n",
    "\n",
    "In practice, WordPiece is implemented as a greedy / incremental procedure where candidate merges (subwords) are added step by step, based on how much they improve the likelihood of the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935c35c",
   "metadata": {},
   "source": [
    "Before running WordPiece:\n",
    "\n",
    "1. Normalize text (e.g., Unicode NFKC, remove accents if desired).\n",
    "\n",
    "2. Decide whether to lowercase (BERT-base uncased lowercased everything).\n",
    "\n",
    "3. Perform initial whitespace tokenization (split into words).\n",
    "\n",
    "4. Start the vocabulary with all individual characters so that no input ever becomes completely unknown."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
