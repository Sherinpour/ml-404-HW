{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c89e6a",
   "metadata": {},
   "source": [
    "**WordPiece**\n",
    "\n",
    "The goal of WordPiece is to build a fixed vocabulary \\(V\\) of subwords such that the likelihood of the training corpus under a simple model is maximized.\n",
    "\n",
    "Formally, if a piece of text is segmented into tokens \\(t_1, t_2, \\ldots, t_m\\), the model assumes:\n",
    "\n",
    "$$\n",
    "P(\\text{text}) \\approx \\prod_{i=1}^{m} P(t_i)\n",
    "$$\n",
    "\n",
    "We want to find a vocabulary \\(V\\) and probabilities \\(P(t)\\) that maximize the likelihood of the training data (or equivalently minimize the negative log-likelihood).\n",
    "\n",
    "In practice, WordPiece is implemented as a greedy / incremental procedure where candidate merges (subwords) are added step by step, based on how much they improve the likelihood of the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935c35c",
   "metadata": {},
   "source": [
    "Before running WordPiece:\n",
    "\n",
    "1. Normalize text (e.g., Unicode NFKC, remove accents if desired).\n",
    "\n",
    "2. Decide whether to lowercase (BERT-base uncased lowercased everything).\n",
    "\n",
    "3. Perform initial whitespace tokenization (split into words).\n",
    "\n",
    "4. Start the vocabulary with all individual characters so that no input ever becomes completely unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a871e813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "['play', '##e', '##r', '##s']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict, Iterable, List, Sequence, Set, Tuple\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Text normalization utilities\n",
    "# -----------------------------\n",
    "\n",
    "def normalize_text(text: str, lowercase: bool = True) -> str:\n",
    "    \"\"\"Normalize unicode (NFKC) and optionally lowercase the text.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    return text.lower() if lowercase else text\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Split text on whitespace into word tokens (keeps only non-empty tokens).\"\"\"\n",
    "    return [t for t in re.split(r\"\\s+\", text) if t]\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# WordPiece core: segmentation and training loop\n",
    "# ---------------------------------------------\n",
    "\n",
    "SPECIAL_TOKENS = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "\n",
    "def split_word_into_char_pieces(word: str) -> List[str]:\n",
    "    \"\"\"Initial segmentation: first character is standalone, rest are continuation with '##'.\"\"\"\n",
    "    if not word:\n",
    "        return []\n",
    "    pieces: List[str] = [word[0]]\n",
    "    for ch in word[1:]:\n",
    "        pieces.append(\"##\" + ch)\n",
    "    return pieces\n",
    "\n",
    "\n",
    "def initial_segmentation(words: Sequence[str]) -> List[List[str]]:\n",
    "    \"\"\"Segment every word into character-level WordPiece pieces.\"\"\"\n",
    "    return [split_word_into_char_pieces(w) for w in words]\n",
    "\n",
    "\n",
    "def count_tokens(segmentation: Sequence[Sequence[str]]) -> Counter:\n",
    "    \"\"\"Count token frequencies across all segmented words.\"\"\"\n",
    "    counts: Counter = Counter()\n",
    "    for pieces in segmentation:\n",
    "        counts.update(pieces)\n",
    "    return counts\n",
    "\n",
    "\n",
    "def compute_log_likelihood(counts: Counter) -> float:\n",
    "    \"\"\"Compute log-likelihood under MLE p(t) = count(t)/sum(counts).\"\"\"\n",
    "    total = sum(counts.values())\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    loglik = 0.0\n",
    "    for tok, c in counts.items():\n",
    "        p = c / total\n",
    "        loglik += c * math.log(max(p, 1e-12))\n",
    "    return loglik\n",
    "\n",
    "\n",
    "def collect_candidate_substrings(\n",
    "    words: Sequence[str],\n",
    "    vocab: Set[str],\n",
    "    min_freq: int,\n",
    "    max_subword_len: int,\n",
    ") -> Counter:\n",
    "    \"\"\"\n",
    "    Collect candidate substrings inside words. For position i==0 we consider raw substring,\n",
    "    for i>0 we consider its '##' prefixed form. We return only those not already in vocab.\n",
    "    \"\"\"\n",
    "    candidates: Counter = Counter()\n",
    "    for word in words:\n",
    "        n = len(word)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, min(n, i + max_subword_len) + 1):\n",
    "                piece = word[i:j]\n",
    "                cand = piece if i == 0 else \"##\" + piece\n",
    "                if cand in vocab:\n",
    "                    continue\n",
    "                candidates[cand] += 1\n",
    "    # filter by minimum frequency\n",
    "    filtered = Counter({tok: c for tok, c in candidates.items() if c >= min_freq})\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def simulate_replace_and_count(\n",
    "    words: Sequence[str],\n",
    "    candidate: str,\n",
    ") -> Counter:\n",
    "    \"\"\"\n",
    "    Simulate adding a candidate token by re-segmenting all words using characters + this candidate.\n",
    "    We use a left-to-right greedy pass per word. Returns the resulting token counts.\n",
    "    \"\"\"\n",
    "    counts: Counter = Counter()\n",
    "\n",
    "    # Candidate specifics\n",
    "    if candidate.startswith(\"##\"):\n",
    "        cand_raw = candidate[2:]\n",
    "    else:\n",
    "        cand_raw = candidate\n",
    "\n",
    "    for word in words:\n",
    "        i = 0\n",
    "        n = len(word)\n",
    "        first = True\n",
    "        while i < n:\n",
    "            # decide whether candidate can match here\n",
    "            if first and not candidate.startswith(\"##\"):\n",
    "                # try to match non-prefixed candidate at start position\n",
    "                if word.startswith(cand_raw, i):\n",
    "                    counts[candidate] += 1\n",
    "                    i += len(cand_raw)\n",
    "                    first = False\n",
    "                    continue\n",
    "            elif (not first) and candidate.startswith(\"##\"):\n",
    "                # try to match continued candidate at non-initial position\n",
    "                if word.startswith(cand_raw, i):\n",
    "                    counts[candidate] += 1\n",
    "                    i += len(cand_raw)\n",
    "                    first = False\n",
    "                    continue\n",
    "            # fallback to emitting a single char piece\n",
    "            ch = word[i]\n",
    "            tok = ch if first else \"##\" + ch\n",
    "            counts[tok] += 1\n",
    "            i += 1\n",
    "            first = False\n",
    "    return counts\n",
    "\n",
    "\n",
    "def build_initial_vocab(words: Sequence[str]) -> Set[str]:\n",
    "    \"\"\"Create initial vocabulary from characters (both standalone and '##' forms) plus special tokens.\"\"\"\n",
    "    chars: Set[str] = set()\n",
    "    for w in words:\n",
    "        chars.update(list(w))\n",
    "    vocab: Set[str] = set(SPECIAL_TOKENS)\n",
    "    for ch in chars:\n",
    "        vocab.add(ch)\n",
    "        vocab.add(\"##\" + ch)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def train_wordpiece(\n",
    "    corpus_texts: Iterable[str],\n",
    "    vocab_target_size: int,\n",
    "    min_freq: int = 2,\n",
    "    max_subword_len: int = 12,\n",
    "    lowercase: bool = True,\n",
    ") -> Tuple[Set[str], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Train a simple WordPiece vocabulary using a greedy likelihood-improvement proxy.\n",
    "\n",
    "    - Normalize and whitespace-tokenize input texts\n",
    "    - Start from character-level segmentation\n",
    "    - Iteratively add the candidate token that maximizes log-likelihood under MLE\n",
    "    \"\"\"\n",
    "    # 1) Normalize and split to words\n",
    "    words: List[str] = []\n",
    "    for text in corpus_texts:\n",
    "        text = normalize_text(text, lowercase=lowercase)\n",
    "        words.extend(whitespace_tokenize(text))\n",
    "    words = [w for w in words if w]\n",
    "\n",
    "    if not words:\n",
    "        # Empty corpus → return just special tokens\n",
    "        vocab = set(SPECIAL_TOKENS)\n",
    "        probs = {tok: 1.0 / len(vocab) for tok in vocab}\n",
    "        return vocab, probs\n",
    "\n",
    "    # 2) Initialize vocabulary with characters and specials\n",
    "    vocab: Set[str] = build_initial_vocab(words)\n",
    "\n",
    "    # 3) Initial segmentation and counts\n",
    "    segmentation = initial_segmentation(words)\n",
    "    counts = count_tokens(segmentation)\n",
    "    current_loglik = compute_log_likelihood(counts)\n",
    "\n",
    "    # Greedy loop\n",
    "    while len(vocab) < vocab_target_size:\n",
    "        candidates = collect_candidate_substrings(\n",
    "            words=words, vocab=vocab, min_freq=min_freq, max_subword_len=max_subword_len\n",
    "        )\n",
    "        if not candidates:\n",
    "            break\n",
    "\n",
    "        best_gain = float(\"-inf\")\n",
    "        best_candidate = None\n",
    "\n",
    "        # Evaluate each candidate by simulated re-segmentation\n",
    "        for cand in candidates:\n",
    "            new_counts = simulate_replace_and_count(words, cand)\n",
    "            new_loglik = compute_log_likelihood(new_counts)\n",
    "            gain = new_loglik - current_loglik\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_candidate = cand\n",
    "\n",
    "        if best_candidate is None or best_gain <= 0:\n",
    "            # No candidate improves likelihood → stop early\n",
    "            break\n",
    "\n",
    "        # Commit best candidate: rebuild counts from simulation to stay consistent\n",
    "        vocab.add(best_candidate)\n",
    "        counts = simulate_replace_and_count(words, best_candidate)\n",
    "        current_loglik = compute_log_likelihood(counts)\n",
    "\n",
    "    # Convert counts to probabilities (MLE)\n",
    "    total = sum(counts.values()) or 1\n",
    "    probs: Dict[str, float] = {t: c / total for t, c in counts.items()}\n",
    "    return vocab, probs\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# Inference: greedy tokenization API\n",
    "# --------------------------------\n",
    "\n",
    "def tokenize_word(word: str, vocab: Set[str], unk_token: str = \"[UNK]\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Greedy longest-match WordPiece tokenization for a single word.\n",
    "    For non-initial pieces we try '##' + piece first, then fallback to characters.\n",
    "    \"\"\"\n",
    "    if not word:\n",
    "        return []\n",
    "\n",
    "    i = 0\n",
    "    tokens: List[str] = []\n",
    "    n = len(word)\n",
    "    while i < n:\n",
    "        end = n\n",
    "        found = False\n",
    "        while end > i:\n",
    "            piece = word[i:end]\n",
    "            if i > 0:\n",
    "                piece_with_prefix = \"##\" + piece\n",
    "                if piece_with_prefix in vocab:\n",
    "                    tokens.append(piece_with_prefix)\n",
    "                    i = end\n",
    "                    found = True\n",
    "                    break\n",
    "            if piece in vocab:\n",
    "                tokens.append(piece)\n",
    "                i = end\n",
    "                found = True\n",
    "                break\n",
    "            end -= 1\n",
    "        if not found:\n",
    "            # If nothing matches, emit [UNK] and stop for this word\n",
    "            return [unk_token]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(text: str, vocab: Set[str], lowercase: bool = True, unk_token: str = \"[UNK]\") -> List[str]:\n",
    "    \"\"\"Tokenize full text: normalize → split into words → apply greedy WordPiece per word.\"\"\"\n",
    "    text = normalize_text(text, lowercase=lowercase)\n",
    "    words = whitespace_tokenize(text)\n",
    "    output: List[str] = []\n",
    "    for w in words:\n",
    "        output.extend(tokenize_word(w, vocab, unk_token=unk_token))\n",
    "    return output\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Minimal usage example\n",
    "# -----------------------\n",
    "# Note: Uncomment to try in the notebook.\n",
    "corpus = [\n",
    "    \"Playing players play playful plays\",\n",
    "    \"Playground played\",\n",
    "]\n",
    "vocab, probs = train_wordpiece(corpus, vocab_target_size=200, min_freq=2, max_subword_len=10)\n",
    "print(len(vocab))\n",
    "print(tokenize(\"players\", vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbce0a2",
   "metadata": {},
   "source": [
    "✅ Strengths\n",
    "\n",
    "1. Greatly reduces OOV (Out-of-Vocabulary) issues\n",
    "\n",
    "    Any word can be split into subwords or even characters, so [UNK] is rare.\n",
    "\n",
    "    Very useful for morphologically rich languages (like Persian, German).\n",
    "\n",
    "2. Smaller and more efficient vocabulary\n",
    "\n",
    "    Instead of millions of words, ~30k subwords are usually enough.\n",
    "\n",
    "    Requires less memory and computation.\n",
    "\n",
    "3. Generalizes to unseen words\n",
    "\n",
    "    New words (e.g., playfulness) can be segmented as play + ##ful + ##ness.\n",
    "\n",
    "    Handles neologisms and inflections better.\n",
    "\n",
    "4. More probabilistic than BPE\n",
    "\n",
    "    Merges are chosen based on likelihood improvement, not just raw frequency.\n",
    "\n",
    "    Produces vocabularies more aligned with the data distribution.\n",
    "\n",
    "5. Supports multilingual settings better\n",
    "\n",
    "    Subwords can be shared across languages.\n",
    "\n",
    "    Example: Multilingual BERT uses WordPiece.\n",
    "\n",
    "❌ Weaknesses\n",
    "\n",
    "1. High computational cost for training\n",
    "\n",
    "    Choosing merges by maximizing likelihood requires repeated re-segmentation and log-likelihood computation.\n",
    "\n",
    "    Much slower than BPE.\n",
    "\n",
    "2. Deterministic, no segmentation diversity\n",
    "\n",
    "    Always uses greedy longest-match-first during inference.\n",
    "\n",
    "    Unlike Unigram/SentencePiece, which can sample multiple segmentations (useful for data augmentation).\n",
    "\n",
    "3. Strongly depends on preprocessing\n",
    "\n",
    "    Lowercasing, normalization, and whitespace splitting heavily affect results.\n",
    "\n",
    "    Problematic for languages without clear word boundaries (Chinese, Persian with half-spaces, etc.).\n",
    "\n",
    "4. Not globally optimal\n",
    "\n",
    "    Greedy selection of merges means the final vocabulary may not be the true optimum.\n",
    "\n",
    "5. Longer sequences in some languages\n",
    "\n",
    "    For highly inflected or agglutinative languages (Turkish, Finnish, Persian), words may split into many subwords.\n",
    "\n",
    "6. Longer sequences → more expensive for Transformers (complexity o(n**2))\n",
    "\n",
    "Subwords often lack clear semantic meaning\n",
    "\n",
    "Tokens like ##ing, ##tion carry little standalone meaning.\n",
    "\n",
    "Can make interpretation harder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab47353",
   "metadata": {},
   "source": [
    "### Libraries and language models that use WordPiece\n",
    "\n",
    "- **Libraries**\n",
    "  - **Hugging Face Transformers/Tokenizers**: Implements WordPiece tokenization (and others) and provides pretrained tokenizers.\n",
    "  - **TensorFlow Text**: WordPiece ops for TF models (used with BERT-style models).\n",
    "  - **KerasNLP**: WordPiece tokenizer layers compatible with Keras models.\n",
    "\n",
    "- **Language models (WordPiece-based)**\n",
    "  - **BERT family**: `bert-base-uncased`, `bert-base-cased`, `bert-large-*`, `bert-base-multilingual-uncased/cased` (mBERT), `BioBERT`, etc.\n",
    "  - **DistilBERT**: `distilbert-base-uncased` (distilled BERT; still WordPiece).\n",
    "  - **ELECTRA**: `google/electra-*` models use WordPiece.\n",
    "  - **MobileBERT**: `google/mobilebert-*` uses WordPiece.\n",
    "  - **LaBSE** and many multilingual encoders derived from BERT use WordPiece.\n",
    "\n",
    "- Models that do NOT use WordPiece (for contrast)\n",
    "  - **RoBERTa** and **GPT-2/3**: BPE (Byte-Pair Encoding).\n",
    "  - **ALBERT** and many Google/XLNet variants may use **SentencePiece (Unigram)**.\n",
    "\n",
    "This is why tokenization behavior (subword splits, vocabulary size) can differ across otherwise similar encoders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf8ede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sherin/miniconda3/envs/exercise_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> BERT (WordPiece) [WordPiece]\n",
      "Tokens ( 9 ):\n",
      "['playing', 'players', 'play', 'playful', 'plays', 'in', 'playground', '##s', '.']\n",
      "\n",
      "==> DistilBERT (WordPiece) [WordPiece]\n",
      "Tokens ( 9 ):\n",
      "['playing', 'players', 'play', 'playful', 'plays', 'in', 'playground', '##s', '.']\n",
      "\n",
      "==> ELECTRA (WordPiece) [WordPiece]\n",
      "Tokens ( 9 ):\n",
      "['playing', 'players', 'play', 'playful', 'plays', 'in', 'playground', '##s', '.']\n",
      "\n",
      "==> RoBERTa (BPE) [BPE]\n",
      "Tokens ( 9 ):\n",
      "['Playing', 'Ġplayers', 'Ġplay', 'Ġplayful', 'Ġplays', 'Ġin', 'Ġplayground', 's', '.']\n",
      "\n",
      "==> ALBERT (SentencePiece) [SentencePiece]\n",
      "Tokens ( 9 ):\n",
      "['▁playing', '▁players', '▁play', '▁playful', '▁plays', '▁in', '▁playground', 's', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparison of tokenizers on the same input\n",
    "# - WordPiece (BERT, DistilBERT, ELECTRA)\n",
    "# - BPE (RoBERTa)\n",
    "# - Unigram/SentencePiece (ALBERT)\n",
    "\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "@dataclass\n",
    "class TokenizerSpec:\n",
    "    name: str\n",
    "    model_id: str\n",
    "    family: str\n",
    "\n",
    "\n",
    "def compare_tokenizers(text: str, specs: List[TokenizerSpec]) -> List[Dict]:\n",
    "    results = []\n",
    "    for spec in specs:\n",
    "        tok = AutoTokenizer.from_pretrained(spec.model_id)\n",
    "        # Use fast path when available\n",
    "        enc = tok(text, add_special_tokens=False)\n",
    "        pieces = tok.convert_ids_to_tokens(enc[\"input_ids\"]) if hasattr(tok, \"convert_ids_to_tokens\") else enc[\"input_ids\"]\n",
    "        results.append({\n",
    "            \"name\": spec.name,\n",
    "            \"family\": spec.family,\n",
    "            \"model_id\": spec.model_id,\n",
    "            \"num_tokens\": len(pieces),\n",
    "            \"tokens\": pieces,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"Playing players play playful plays in playgrounds.\"\n",
    "\n",
    "    SPECS = [\n",
    "        # WordPiece\n",
    "        TokenizerSpec(name=\"BERT (WordPiece)\", model_id=\"bert-base-uncased\", family=\"WordPiece\"),\n",
    "        TokenizerSpec(name=\"DistilBERT (WordPiece)\", model_id=\"distilbert-base-uncased\", family=\"WordPiece\"),\n",
    "        TokenizerSpec(name=\"ELECTRA (WordPiece)\", model_id=\"google/electra-base-discriminator\", family=\"WordPiece\"),\n",
    "        # BPE\n",
    "        TokenizerSpec(name=\"RoBERTa (BPE)\", model_id=\"roberta-base\", family=\"BPE\"),\n",
    "        # SentencePiece/Unigram\n",
    "        TokenizerSpec(name=\"ALBERT (SentencePiece)\", model_id=\"albert-base-v2\", family=\"SentencePiece\"),\n",
    "    ]\n",
    "\n",
    "    rows = compare_tokenizers(sample_text, SPECS)\n",
    "\n",
    "    # Pretty print comparison\n",
    "    for row in rows:\n",
    "        print(\"==>\", row[\"name\"], f\"[{row['family']}]\")\n",
    "        print(\"Tokens (\", row[\"num_tokens\"], \"):\")\n",
    "        print(row[\"tokens\"])\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b95a2c",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE builds a fixed subword vocabulary by iteratively merging the most frequent adjacent symbol pairs in a corpus.\n",
    "\n",
    "- Start with each word split into characters plus an end-of-word marker (e.g., `</w>`)\n",
    "- Count pair frequencies across the corpus\n",
    "- Merge the most frequent pair into a new symbol; repeat until reaching the target vocab size\n",
    "\n",
    "Given a word, tokenization applies the learned merges greedily to reconstruct subword units.\n",
    "\n",
    "Compared to WordPiece, BPE chooses merges by raw pair frequency rather than maximizing likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f85e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical BPE implementation (training + tokenization)\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "import re\n",
    "\n",
    "EOW = \"</w>\"\n",
    "\n",
    "\n",
    "def get_vocab(words: Iterable[str]) -> Counter:\n",
    "    vocab = Counter()\n",
    "    for w in words:\n",
    "        # word as list of chars + end-of-word\n",
    "        pieces = list(w) + [EOW]\n",
    "        vocab[tuple(pieces)] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def get_pair_frequencies(vocab: Counter) -> Dict[Tuple[str, str], int]:\n",
    "    pair_freq: Dict[Tuple[str, str], int] = defaultdict(int)\n",
    "    for word_tuple, count in vocab.items():\n",
    "        for i in range(len(word_tuple) - 1):\n",
    "            pair = (word_tuple[i], word_tuple[i + 1])\n",
    "            pair_freq[pair] += count\n",
    "    return pair_freq\n",
    "\n",
    "\n",
    "def merge_pair_in_vocab(vocab: Counter, pair_to_merge: Tuple[str, str]) -> Counter:\n",
    "    new_vocab: Counter = Counter()\n",
    "    bigram = re.escape(\" \".join(pair_to_merge))\n",
    "    pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "    for word_tuple, count in vocab.items():\n",
    "        symbols = list(word_tuple)\n",
    "        # Convert to space-separated symbols to replace bigram\n",
    "        s = \" \".join(symbols)\n",
    "        merged_symbol = pair_to_merge[0] + pair_to_merge[1]\n",
    "        s = re.sub(pattern, merged_symbol, s)\n",
    "        new_symbols = tuple(s.split(\" \"))\n",
    "        new_vocab[new_symbols] += count\n",
    "    return new_vocab\n",
    "\n",
    "\n",
    "def train_bpe(words: Iterable[str], num_merges: int) -> List[Tuple[str, str]]:\n",
    "    vocab = get_vocab(words)\n",
    "    merges: List[Tuple[str, str]] = []\n",
    "    for _ in range(num_merges):\n",
    "        pair_freq = get_pair_frequencies(vocab)\n",
    "        if not pair_freq:\n",
    "            break\n",
    "        best_pair, best_count = max(pair_freq.items(), key=lambda x: x[1])\n",
    "        if best_count <= 0:\n",
    "            break\n",
    "        merges.append(best_pair)\n",
    "        vocab = merge_pair_in_vocab(vocab, best_pair)\n",
    "    return merges\n",
    "\n",
    "\n",
    "def apply_bpe_tokenize(word: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    symbols = list(word) + [EOW]\n",
    "    # Build a quick lookup for merges: left -> possible rights with rank\n",
    "    merge_ranks: Dict[Tuple[str, str], int] = {pair: i for i, pair in enumerate(merges)}\n",
    "\n",
    "    while True:\n",
    "        # Find all adjacent pairs\n",
    "        pairs = [(symbols[i], symbols[i + 1]) for i in range(len(symbols) - 1)]\n",
    "        # Rank them\n",
    "        candidate_pairs = [(merge_ranks.get(p, float(\"inf\")), p) for p in pairs]\n",
    "        best_rank, best_pair = min(candidate_pairs, key=lambda x: x[0])\n",
    "        if best_rank == float(\"inf\"):\n",
    "            break\n",
    "        # Merge first occurrence of best_pair\n",
    "        new_symbols: List[str] = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and (symbols[i], symbols[i + 1]) == best_pair:\n",
    "                new_symbols.append(symbols[i] + symbols[i + 1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        symbols = new_symbols\n",
    "    # Drop end-of-word marker\n",
    "    if symbols and symbols[-1] == EOW:\n",
    "        symbols = symbols[:-1]\n",
    "    return symbols\n",
    "\n",
    "\n",
    "def bpe_tokenize(text: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    tokens: List[str] = []\n",
    "    for word in re.findall(r\"\\S+\", text):\n",
    "        tokens.extend(apply_bpe_tokenize(word, merges))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Minimal usage example\n",
    "if __name__ == \"__main__\":\n",
    "    corpus = [\n",
    "        \"Playing players play playful plays\",\n",
    "        \"Playground played\",\n",
    "    ]\n",
    "    # Build a small merge list for demo\n",
    "    words = [w.lower() for line in corpus for w in re.findall(r\"\\S+\", line)]\n",
    "    merges = train_bpe(words, num_merges=50)\n",
    "    print(\"#merges:\", len(merges))\n",
    "    print(bpe_tokenize(\"players\", merges))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a8235",
   "metadata": {},
   "source": [
    "✅ Strengths (BPE)\n",
    "\n",
    "1. **Simple and fast to train**\n",
    "   - Only counts pair frequencies and merges most frequent pairs.\n",
    "\n",
    "2. **Reduces OOVs**\n",
    "   - Any word can be split into frequent subwords/characters.\n",
    "\n",
    "3. **Compact vocabularies**\n",
    "   - 30k–50k merges are usually enough for many languages.\n",
    "\n",
    "4. **Deterministic inference**\n",
    "   - Greedy application of merges yields consistent tokenization.\n",
    "\n",
    "❌ Weaknesses (BPE)\n",
    "\n",
    "1. **No likelihood objective**\n",
    "   - Chooses merges purely by frequency, not by maximizing corpus likelihood.\n",
    "\n",
    "2. **May overfit frequent pairs**\n",
    "   - Common bigrams dominate early; rare but semantically meaningful units can be missed.\n",
    "\n",
    "3. **Whitespace and normalization sensitive**\n",
    "   - Preprocessing choices strongly affect learned merges.\n",
    "\n",
    "4. **Subword boundary artifacts**\n",
    "   - BPE encodings (like RoBERTa) use special markers (e.g., `Ġ`) to indicate word starts, which can be unintuitive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6cfe3",
   "metadata": {},
   "source": [
    "### Libraries and models that use BPE\n",
    "\n",
    "- **Libraries**\n",
    "  - **Hugging Face Tokenizers/Transformers**: BPE training/decoding (e.g., RoBERTa, GPT-2 style)\n",
    "  - **SentencePiece**: Implements BPE and Unigram (used by many Google models)\n",
    "\n",
    "- **Language models (BPE-based)**\n",
    "  - **RoBERTa** (`roberta-base`, `xlm-roberta-base` uses a BPE-like variant over bytes)\n",
    "  - **GPT-2** and many GPT derivatives (OpenAI GPT-2 BPE)\n",
    "  - **Llama 1/2/3** use a byte-level BPE variant\n",
    "  - Many tokenizers described as \"byte-level BPE\" fall in this family\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe787147",
   "metadata": {},
   "source": [
    "### SentencePiece (Unigram)\n",
    "\n",
    "SentencePiece is a language-independent subword tokenizer that learns directly from raw text without external tokenization. Its most common mode uses a Unigram language model to select a subword vocabulary and segmentations.\n",
    "\n",
    "Core ideas\n",
    "- Operates on raw text (often bytes or Unicode codepoints), not on pre-tokenized words\n",
    "- Learns a subword vocabulary and probabilities under a Unigram LM\n",
    "- At inference, segments text by maximizing the likelihood under the Unigram model (can also sample for augmentation)\n",
    "- Uses special markers, e.g., `▁` to indicate word boundaries in many models\n",
    "\n",
    "Variants\n",
    "- `model_type=\"unigram\"` (default, probabilistic; supports sampling)\n",
    "- `model_type=\"bpe\"` (BPE training inside SentencePiece)\n",
    "- `model_type=\"char\"`, `word` for other baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44612a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal SentencePiece Unigram training + tokenization demo\n",
    "# Requires: pip install sentencepiece\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from typing import List\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "CORPUS = [\n",
    "    \"Playing players play playful plays\",\n",
    "    \"Playground played\",\n",
    "]\n",
    "\n",
    "# Train a tiny unigram model on the toy corpus\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    corpus_path = os.path.join(td, \"corpus.txt\")\n",
    "    with open(corpus_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in CORPUS:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "    model_prefix = os.path.join(td, \"sp_demo\")\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=corpus_path,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=200,\n",
    "        model_type=\"unigram\",\n",
    "        character_coverage=1.0,\n",
    "        input_sentence_size=0,\n",
    "        shuffle_input_sentence=False,\n",
    "    )\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(model_prefix + \".model\")\n",
    "\n",
    "    sample_text = \"Playing players play playful plays in playgrounds.\"\n",
    "    ids: List[int] = sp.encode(sample_text, out_type=int)\n",
    "    pieces: List[str] = sp.id_to_piece(ids)\n",
    "\n",
    "    print(\"SentencePiece (Unigram) tokens:\")\n",
    "    print(pieces)\n",
    "    print(\"num_tokens:\", len(pieces))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33895d73",
   "metadata": {},
   "source": [
    "✅ Strengths (SentencePiece)\n",
    "\n",
    "1. **Language independent**\n",
    "   - No need for whitespace tokenization or language-specific preprocessing.\n",
    "\n",
    "2. **Probabilistic segmentation (Unigram)**\n",
    "   - Can sample multiple segmentations (useful for data augmentation).\n",
    "\n",
    "3. **Great OOV handling**\n",
    "   - Learns subwords directly over raw text; robust to scripts without spaces.\n",
    "\n",
    "4. **Unified tooling**\n",
    "   - Single trainer supports Unigram/BPE/Char, integrates cleanly in pipelines.\n",
    "\n",
    "❌ Weaknesses (SentencePiece)\n",
    "\n",
    "1. **Training time and memory**\n",
    "   - Unigram optimization over large corpora can be heavier than BPE.\n",
    "\n",
    "2. **Interpretability**\n",
    "   - `▁` boundary markers and probabilistic outputs can be less intuitive.\n",
    "\n",
    "3. **Pre/Post-processing differences**\n",
    "   - Models trained with different normalization/coverage settings are not drop-in compatible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06c558",
   "metadata": {},
   "source": [
    "### Libraries and models that use SentencePiece\n",
    "\n",
    "- **Libraries**\n",
    "  - **SentencePiece** (reference C++/Python library)\n",
    "  - **Hugging Face Transformers/Tokenizers** (loads SentencePiece models; many tokenizers depend on `.model` files)\n",
    "\n",
    "- **Language models (SentencePiece-based)**\n",
    "  - **ALBERT** (`albert-base-v2`)\n",
    "  - **XLNet**\n",
    "  - **T5/mT5** (Unigram SentencePiece)\n",
    "  - **MarianMT**\n",
    "  - **PEGASUS**\n",
    "  - Many multilingual encoders/MT models from Google and others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8fd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exercise_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
